{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "udemy_12_spark_functions1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH0Fm8ypLXWIPYBvHWRoHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahesh-y/mahesh-y.github.io/blob/master/udemy_12_spark_functions1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ9fUPTec7zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title load spark in colab\n",
        "#for the most recent update on 02/29/2020\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q -nv https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "# 3.Start Spark Session\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.5-bin-hadoop2.7\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM9ZhsFwf2O3",
        "colab_type": "code",
        "outputId": "ec6c2b8b-b2f2-4317-ea11-c337646ca4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "#@title create dummy DF\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('test1').getOrCreate()\n",
        "from pyspark.sql import functions as F\n",
        "# create dataframe with one column of a sequence\n",
        "df_start=5; df_len = 100; step =2\n",
        "df1 = spark.range(df_start, df_len, step).toDF(\"id\")\n",
        "df1.show(5)\n",
        "# add multiple columns\n",
        "df1.withColumn(\"id1\",df1[\"id\"] ).withColumn(\"id2\", F.col(\"id\")+F.col(\"id1\")).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  5|\n",
            "|  7|\n",
            "|  9|\n",
            "| 11|\n",
            "| 13|\n",
            "+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+---+---+\n",
            "| id|id1|id2|\n",
            "+---+---+---+\n",
            "|  5|  5| 10|\n",
            "|  7|  7| 14|\n",
            "|  9|  9| 18|\n",
            "| 11| 11| 22|\n",
            "| 13| 13| 26|\n",
            "+---+---+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP93RRIeNYr5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW1xACi-jMhg",
        "colab_type": "code",
        "outputId": "0e9b1923-e41a-4b25-f1de-f4b2b487cf01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "#@title stack many DF\n",
        "df1 = spark.createDataFrame([('apple','ball','ballon'),('cat','camel','james'),('none','focus','cake')],[\"col1\", \"col2\", \"col3\"])\n",
        "df1.show()\n",
        "df1.union(df1).show()\n",
        "\n",
        "df2_schema = df1.schema\n",
        "df2 = spark.createDataFrame([], df2_schema)\n",
        "for ii in range(5):\n",
        "  df2 = df2.union(df1)\n",
        "  print(df2.count())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+------+\n",
            "| col1| col2|  col3|\n",
            "+-----+-----+------+\n",
            "|apple| ball|ballon|\n",
            "|  cat|camel| james|\n",
            "| none|focus|  cake|\n",
            "+-----+-----+------+\n",
            "\n",
            "+-----+-----+------+\n",
            "| col1| col2|  col3|\n",
            "+-----+-----+------+\n",
            "|apple| ball|ballon|\n",
            "|  cat|camel| james|\n",
            "| none|focus|  cake|\n",
            "|apple| ball|ballon|\n",
            "|  cat|camel| james|\n",
            "| none|focus|  cake|\n",
            "+-----+-----+------+\n",
            "\n",
            "3\n",
            "6\n",
            "9\n",
            "12\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3QRAfAkrQqT",
        "colab_type": "code",
        "outputId": "5229eea5-6507-4db4-ccb9-3399e489a6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "#@title add seq column to DF\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql import Window\n",
        "df2 = df1.withColumn(\n",
        "    \"index\",\n",
        "    f.row_number().over(Window.orderBy(f.monotonically_increasing_id()))\n",
        ")\n",
        "df2.show()\n",
        "df2.withColumn(\"index_sq\", df2['index']*2).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+------+-----+\n",
            "| col1| col2|  col3|index|\n",
            "+-----+-----+------+-----+\n",
            "|apple| ball|ballon|    1|\n",
            "|  cat|camel| james|    2|\n",
            "| none|focus|  cake|    3|\n",
            "+-----+-----+------+-----+\n",
            "\n",
            "+-----+-----+------+-----+--------+\n",
            "| col1| col2|  col3|index|index_sq|\n",
            "+-----+-----+------+-----+--------+\n",
            "|apple| ball|ballon|    1|       2|\n",
            "|  cat|camel| james|    2|       4|\n",
            "| none|focus|  cake|    3|       6|\n",
            "+-----+-----+------+-----+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl715uRwrR9w",
        "colab_type": "code",
        "outputId": "70f87178-c222-402c-b26b-1b9df6465af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "#@title add rand/normal to DF\n",
        "from pyspark.sql import functions as f\n",
        "df1 = spark.f.rand().toDF(\"id\")\n",
        "df1.show(5)\n",
        "# add multiple columns\n",
        "df1.withColumn(\"id1\",df1[\"id\"] ).withColumn(\"id2\", F.col(\"id\")+F.col(\"id1\")).show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-06167f8d020c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# add multiple columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'f'"
          ]
        }
      ]
    }
  ]
}